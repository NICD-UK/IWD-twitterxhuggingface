{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the HuggingFace `pipeline` class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of sentiment analysis, but most commonly we wish to classify data into positive, negative or neutral classes. \n",
    "\n",
    "We might use sentiment analysis to:\n",
    "\n",
    "- analyse tweets or other social media mentions - e.g. to compare with competitors\n",
    "- get insights into what customers do and don't like about a product or service\n",
    "- detect negative reviews quickly so action can be taken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use pre-trained models from HuggingFace to run analyse sentiments in twitter data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the twitter data, let's see how easy it can be to use these models!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to run some setup, to install the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can import the pipeline class, which provides a super easy interface for making predictions using models from the HuggingFace hub. Let's select the default model for sentiment analysis.\n",
    "\n",
    "[Pipelines documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline('sentiment-analysis')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the model set up, we can go ahead and try it! Update the text below to try classifying different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"I love NLP\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily specify a particular model in the pipeline setup that is more appropriate for your use case. For instance\n",
    "\n",
    "- [`twitter-roberta-base-sentiment`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment) from `cardiffnlp` is trained on tweets and fine-tuned for sentiment analysis\n",
    "- [`bert-base-multilingual-uncased-sentiment`](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) from `nlptown` is a BERT model fine-tuned for sentiment analysis on product reviews in English, Dutch, German, French, Spanish and Italian\n",
    "- [`distilbert-base-uncased-emotion`](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) from `bhadresh-savani` is fine-tuned for detecting emotions in texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the emotion model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_pipeline = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it with some text of your choice. I've gone for __'Hope' is the thing with feathers__ by Emily Dickinson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"‘Hope’ is the thing with feathers\\\n",
    "    That perches in the soul\\\n",
    "    And sings the tune without the words\\\n",
    "    And never stops – at all\\\n",
    "    And sweetest – in the Gale – is heard\\\n",
    "    And sore must be the storm\\\n",
    "    That could abash the little Bird\\\n",
    "    That kept so many warm\\\n",
    "    I’ve heard it in the chillest land\\\n",
    "    And on the strangest Sea\\\n",
    "    Yet, never, in Extremity,\\\n",
    "    It asked a crumb – of Me.\\\n",
    "\"]\n",
    "emotion_pipeline(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more flexible approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will load in a dataset of tweets, run some preprocessing and use a pre-trained model to classify into positive, negative or neutral sentiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to the data set you wish to use. There are some pre-downloaded data sets on the workshop GitHub in the [data folder](https://github.com/NICD-UK/IWD-twitterxhuggingface/tree/main/data), or feel free to try using your own data!\n",
    "\n",
    "If you have an existing twitter developer account you can use the notebook [`get_tweets.ipynb`](https://github.com/NICD-UK/IWD-twitterxhuggingface/blob/main/get_tweets.ipynb) to get your own twitter data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'https://raw.githubusercontent.com/NICD-UK/IWD-twitterxhuggingface/main/data/elon_musk_tweets.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the tweets into a pandas dataframe and take a look at the top five. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(pd.read_csv(PATH)['text']) \n",
    "tweets_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our twitter data contains lots of web links and usernames. We can write a simple preprocessing script, that will replace all usernames with `@user` and all links with `http`. \n",
    "\n",
    "Have a look at the data and think about what other preprcessing might be useful. Remember that we want to avoid loss of any information that might be informative when classifying sentiments. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can set up the model, tokeniser and config. We will use the [`twitter-roberta-base-sentiment`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment) model from `cardiffnlp` that we mentioned earlier, with a slightly different setup. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The preprocessing step uses our custom preprocessing method, and we will use the model's default tokeniser. \n",
    "\n",
    "We will add both the preprocessed and encoded version of each tweet to our dataframe, to allow further inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tweets = []\n",
    "encoded_tweets = []\n",
    "\n",
    "for tweet in tweets_df['text']:\n",
    "    preprocessed = preprocess(tweet)\n",
    "    encoded = tokenizer(preprocessed, return_tensors='pt')\n",
    "    preprocessed_tweets.append(preprocessed)\n",
    "    encoded_tweets.append(encoded)\n",
    "\n",
    "tweets_df['preprocessed'] = preprocessed_tweets\n",
    "tweets_df['encoded'] = encoded_tweets\n",
    "tweets_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use the model to get classification scores, and use a softmax function to convert the scores into a vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_analysis = []\n",
    "for item in tweets_df.encoded:\n",
    "\n",
    "    output = model(**item)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    tweets_analysis.append(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's join up our analysis with the tweets dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.concat([tweets_df, pd.DataFrame(tweets_analysis)], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns to make them more reader-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.rename(columns={'LABEL_0': 'negative', 'LABEL_1': 'neutral','LABEL_2': 'positive'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a column to our dataframe to specify the sentiment with the maximum probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['sentiment'] = tweets_df[['negative','positive', 'neutral']].idxmax(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a tweet for each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.width', 3000)\n",
    " \n",
    "display(tweets_df[tweets_df[\"sentiment\"] == 'positive'].head(1))\n",
    "display(tweets_df[tweets_df[\"sentiment\"] == 'neutral'].head(1))\n",
    "display(tweets_df[tweets_df[\"sentiment\"] == 'negative'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having classified the tweets, what questions might we want to ask about our data? We might be interested in the distirbution over the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = tweets_df.groupby(['sentiment']).size()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a quick plot to visualise this (for more beautiful plots, you need to be in the Visualisation masterclass with Louise!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "ax = plt.subplot(111)\n",
    "sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst less technical, wordclouds can be a fun and populay way to visualise common terms in text data. We can make two wordclouds, one for positive and one for negative tweets. \n",
    "\n",
    "Remember we are working with a small data set!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the libraries and get our positive and negative tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "positive_tweets = tweets_df['text'][tweets_df[\"sentiment\"] == 'positive']\n",
    "negative_tweets = tweets_df['text'][tweets_df[\"sentiment\"] == 'negative']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets get a list of stopwords to remove. These are commonly used words that don't carry meaning on their own. Be careful with these - some are better quality than others! \n",
    "\n",
    "We will add `https`, `co` and `RT` to our list. You might want to add others after looking at the wordclouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells will create the positive and negative wordclouds. \n",
    "\n",
    "**A reminder that the data is real twitter data and has not been filtered for toxicity or profanity!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(positive_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Positive Tweets\")\n",
    "plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(negative_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Negative Tweets\")\n",
    "plt.imshow(negative_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
